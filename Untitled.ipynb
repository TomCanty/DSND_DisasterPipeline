{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\thoma\\\\Documents\\\\Udacity\\\\DSND_Term2\\\\project_files\\\\Disaster Pipeline'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thoma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thoma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pickle\n",
    "\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(re.sub('[^\\w\\s]',' ',text))\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            try: \n",
    "                first_word, first_tag = pos_tags[0]\n",
    "            except:\n",
    "                print(sentence_list,sentence,pos_tags)\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = X.apply(lambda x: self.starting_verb(x))\n",
    "        #X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "    pass\n",
    "\n",
    "class StartingAdjExtractor(BaseEstimator, TransformerMixin):\n",
    "    def starting_adj(self, text):\n",
    "        sentence_list = nltk.sent_tokenize(re.sub('[^\\w\\s]',' ',text))\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['JJ'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = X.apply(lambda x: self.starting_adj(x))\n",
    "        #X_tagged = pd.Series(X).apply(self.starting_adj)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "    pass\n",
    "\n",
    "def load_data(database_filepath):\n",
    "    dbpath = 'sqlite:///' + database_filepath\n",
    "    engine = create_engine(dbpath)\n",
    "    df = pd.read_sql('SELECT * FROM \"Messages\"', engine)\n",
    "    X = df['message']\n",
    "    Y = df.drop(columns=['message','original','genre'])\n",
    "\n",
    "    return X,Y, list(Y.columns)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(re.sub('[^\\w\\s]',' ',text))\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lem.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "       \n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def build_model():\n",
    "       \n",
    "    pipeline = Pipeline([\n",
    "        ('features',FeatureUnion([\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "            ('starting_verb', StartingVerbExtractor()),\n",
    "            ('starting_adj', StartingAdjExtractor())\n",
    "        ])),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "        ])\n",
    "    \n",
    "    parameters = {'clf__estimator__max_depth': [500, 750, None],\n",
    "    'clf__estimator__max_features': ['auto'],\n",
    "    'clf__estimator__min_samples_leaf': [1, 3],\n",
    "    'clf__estimator__min_samples_split': [2, 5, 10]}\n",
    "\n",
    "    cv = GridSearchCV(pipeline,parameters)\n",
    "    \n",
    "    return cv\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    Y_predict = model.predict(X_test)\n",
    "    for i in range(Y_test.shape[1]):\n",
    "        print(category_names[i])\n",
    "        print(classification_report(Y_predict[:,i],Y_test.values[:,i]))\n",
    "    pass\n",
    "\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    pickle.dump(model, open(model_filepath, 'wb'))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_filepath = \"Messages.db\"\n",
    "\n",
    "model_filepath = \"model1.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, category_names = load_data(database_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "blah = StartingVerbExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1712     I would like some information about immigratng...\n",
       "15975    The possibility of a strong aftershock measuri...\n",
       "5904     WE ARE IN FERMAT FEJAK DONTE WE NEED FOOD WATE...\n",
       "213      I need some medical equipment and food in the ...\n",
       "712      I will no longer bother you, I have so much pr...\n",
       "1111     I need your help. My house collapsed. I lost e...\n",
       "16234    More rains have been forecast in the next few ...\n",
       "21533    The announcement came during her trip to Sierr...\n",
       "14015    The Australian Government will contribute $250...\n",
       "10905               Bottled water and other personal items\n",
       "11304    Winter Jackets , Blankets , Towels , Some food...\n",
       "25581    It is also developing market gardens to improv...\n",
       "9135     Digicel has been the first compagny in Haiti o...\n",
       "9855                NOTES: it's not important to translate\n",
       "13859    The Fucheng ICBC sub-branch in Mianyang was fo...\n",
       "3167     What is the price of food right now like rice ...\n",
       "6373     Does the minister of the national education or...\n",
       "10071    RT troylivesay The Livesay Haiti Weblog God He...\n",
       "20787    The government of China in particular has been...\n",
       "7388                     Wesantyahoo.fr.Pepayisenyahoo.fr \n",
       "4781     I already did this. Its saying the Digicel (A ...\n",
       "21738    We are already distributing cooked food and ra...\n",
       "11109    53 matchbooks , several flashlights , a few ba...\n",
       "2349     My friends I can't take it anymore. I have no ...\n",
       "6194     Hello, how are you, please I would like to kno...\n",
       "4579     I am really longing for your tender love. I lo...\n",
       "11482    Death toll at 723 - coastal towns devastated -...\n",
       "9493     Hi! Is this true going in the sounami happenin...\n",
       "12106    There is a big loss for us in the flood. The w...\n",
       "15321    Strong wind, hailstorm and rain hit several mu...\n",
       "                               ...                        \n",
       "6733     I wanted you write to say small to you good ev...\n",
       "6527                   i dont have the tent, you joke me. \n",
       "10912        I don't get to have Halloween this year .. ..\n",
       "7929     we find anything has Delmas 75 Street Berenich...\n",
       "13609    Indian Air Force officers at Car Nicobar docum...\n",
       "12124    near about 50 houses of basti jam gulab pahore...\n",
       "17903    On 8 April the only data reported concerned th...\n",
       "5725                        What can I do to help others? \n",
       "18024    While officials reported 10 casualties and 34 ...\n",
       "13015    RIP to those who were taken from us due to thi...\n",
       "2763     Radio, we are actually counting on your help, ...\n",
       "16724    UN agencies are conducting a crash course on c...\n",
       "8613        I want some information about USAID and DEED. \n",
       "25130    Also not mentioned in the report were the thou...\n",
       "19517    To save Ahmedabad and Rajkot TAX from overload...\n",
       "6779      that what a profesial must make to find an uses \n",
       "3356     We are in a very sad situation in Fontamara 43...\n",
       "9735     I thank you so to have given me the chance to ...\n",
       "14189    As the cyclone struck right before the annual ...\n",
       "7669     I want to know if somebody get a vaginal infec...\n",
       "11157    Safe food/formula for those with severe food a...\n",
       "15535    This assistance will help secure crop harvests...\n",
       "8575             i do not any information for this month. \n",
       "24770    This shipment will contain several tons of med...\n",
       "812      No Location : Good morning in Jesus name. We w...\n",
       "12992    Alright, fuck this storm aftermath shit. Im le...\n",
       "5482     Please send a calling card for me so I can mak...\n",
       "10900    if there is a shelter or food kitchen that nee...\n",
       "1708             i need more information on the earthquake\n",
       "19176    Yesterday, they began assembling additional po...\n",
       "Name: message, Length: 20972, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(X_train['message']).appl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if len(sys.argv) == 3:\n",
    "        database_filepath, model_filepath = sys.argv[1:]\n",
    "        print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        X, Y, category_names = load_data(database_filepath)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        \n",
    "        print('Building model...')\n",
    "        model = build_model()\n",
    "        \n",
    "        print('Training model...')\n",
    "        print('Xtrain: ', X_train.shape)\n",
    "        print('Ytrain: ', Y_train.shape)\n",
    "        print(model)\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        print('Evaluating model...')\n",
    "        evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "        print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('Trained model saved!')\n",
    "\n",
    "    else:\n",
    "        print('Please provide the filepath of the disaster messages database '\\\n",
    "              'as the first argument and the filepath of the pickle file to '\\\n",
    "              'save the model to as the second argument. \\n\\nExample: python '\\\n",
    "              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
